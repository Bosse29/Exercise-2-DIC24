{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501bc548-f2b0-436d-a7a1-6e768c37239e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1) RDDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbba45-e4e7-40bf-8cab-18e978b9dc9c",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df19478-3a3e-4b67-b8e0-5bead0849d30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Loading the data from HDFS\n",
    "// Converting the json file intto a dataframe\n",
    "val Path= \"hdfs:///user/dic24_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "val data = spark.read.json(Path)\n",
    "// Making the RDD \n",
    "// Category is the Key \n",
    "// reviewText is the value\n",
    "val pairs = data.rdd.map{row=>\n",
    "    val category = row.getAs[String] (\"category\")\n",
    "    val text = row.getAs[String] (\"reviewText\")\n",
    "    (category,text)}\n",
    "// \n",
    "// Counting the total number of reviews\n",
    "val N = pairs.count().toInt\n",
    "// Reading stopWord into RDD string and convert it to an array\n",
    "val stopwordsPath = \"Exercise2/stopwords.txt\"\n",
    "val stopwords = sc.textFile(stopwordsPath).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4659c7-e58e-4452-8886-e235811e63a6",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b28b8b-93c8-468b-9a70-21078c160d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// converting the values(reviewText) to lowercase\n",
    "//spliting the reviewTexts into words and removing any non-alphabetic characters\n",
    "//removing stopWords from ReviewTexts\n",
    "val clean_reviews = pairs.mapValues(value=>value.toLowerCase\n",
    "                                  .split(\"[^a-zA-Z]+\")\n",
    "                                  .filterNot(x => stopwords.contains(x.toLowerCase)).mkString(\" \")).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d601d2e7-8034-44d2-b7ac-fcd1937f1e25",
   "metadata": {},
   "source": [
    "## Chi_2 Value Calculating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73d8bb-9482-4d3c-8174-7a92c5e14edf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Calculate a number of unique words in the whole reviews. The result is a tuple (word, frequency of word )\n",
    "// first split the words ,and then we give each word value one an sum them for each word with reduce \n",
    "val countWords = clean_reviews.map{ case (key, value) =>  value }.flatMap (x=>x.split(\" \")).map(word => (word, 1)).reduceByKey ((x,y)=>x+y)\n",
    "// Calculate a frequency of each category\n",
    "// we give each key value one and sum them for each category them with reduce\n",
    "val countCategory = clean_reviews.map{ case (key, value) =>  (key,1)}.reduceByKey ((x,y)=>x+y)\n",
    "\n",
    "//counting number of each words per category\n",
    "//we give each word in each category value one and sum them with reduce for each category,word as the key\n",
    "val categoryTermCount = clean_reviews.flatMapValues(x=>x.split(\" \")).map(word => (word, 1)).reduceByKey ((x,y)=>x+y)\n",
    "// making a tuple with key value of category and value of (word,number of that word in the respective category)\n",
    "val result_A = categoryTermCount.map{case(k,v) => ( (k._2),( k._1, v))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd00860d-efba-4dc6-ab25-a481bc7cb308",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Do the join datasets per word column\n",
    "// tuple:(word,((category,A),countWords))\n",
    "val rddJoin = result_A.join(countWords)\n",
    "// counting B , The data now represents a tuple (category (word, A,B))\n",
    "val result_A_B = rddJoin.map{case (k,v)=> (v._1._1, (k, v._1._2, v._2-v._1._2)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2775e982-8274-4119-98f8-1bb58e6395ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Join per category\n",
    "// tapule\"(category,((word,A,B),countCategory)))\n",
    "val rddJoin2 = result_A_B.join(countCategory).persist\n",
    "print(rddJoin2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729c8835-ec1e-4a85-b5c6-8a5d2c530ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Calculate chi_2 value using A,B,C,D,N\n",
    "val chi2 = rddJoin2.map{case (k,v)=> {\n",
    "    val A = v._1._2.toFloat\n",
    "    val B = v._1._3.toFloat\n",
    "    val C = v._2-A\n",
    "    val D = N - A - B - C\n",
    "    val result = (N*(A*D-B*C)*(A*D-B*C))/((A+B)*(A+C)*(B+D)*(C+D))\n",
    "    (k, (result,v._1._1))\n",
    "}}\n",
    "// Group the lines according to the key (=category) and sort according to the value of chi_2\n",
    "val grouped = chi2.groupByKey().mapValues(tuple => tuple.toList.sortBy(-_._1))\n",
    "// Extract the first 75 values in each category\n",
    "val grouped_75 = grouped.mapValues(line=>line.take(75)).sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea54471f-660c-46b9-858c-e56a38368404",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaaaa70-aa80-48e6-9348-223f3e220471",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scala.reflect.io.File\n",
    "\n",
    "// Transforming and saving the output like the previous exercise\n",
    "val output = grouped_75.map { case (category, terms) =>\n",
    "  val formattedTerms = terms.map { case (term, chi2) =>\n",
    "    s\"$chi2:$term\"\n",
    "  }.mkString(\" \")\n",
    "\n",
    "  s\"<$category> $formattedTerms\"\n",
    "}\n",
    "\n",
    "// Save the output\n",
    "val file = File(\"output_rdd.txt\")\n",
    "file.writeAll(output.collect().mkString(\"\\n\"))\n",
    "\n",
    "// Extract terms and sort them alphabetically\n",
    "val terms = grouped_75.flatMap { case (category, terms) =>\n",
    "  terms.map { case (term, chi2) => chi2 }\n",
    "}.distinct().collect().sorted\n",
    "\n",
    "// Append the sorted terms to the file\n",
    "file.appendAll(\"\\n\" + terms.mkString(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6da3c6-2545-4752-879a-5d7f4eb2cfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81019d4b-20c7-41fc-b1dd-b2c2d76ea697",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
