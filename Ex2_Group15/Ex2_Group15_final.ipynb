{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75c88343-802b-4204-b7b7-8e005146364c",
   "metadata": {},
   "source": [
    "# Exercise 2: Text Processing and Classification using Spark\n",
    "### Data-intensive Computing 2023, Group 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2ef53b-203a-4398-8c58-b5d7c3961355",
   "metadata": {},
   "source": [
    "## Part 1) RDDs\n",
    "\n",
    "Repeat the steps of Assignment 1, i.e. calculation of chi-square values and output of the sorted top terms per category, as well as the joined dictionary, using RDDs and transformations. Write the output to a file output_rdd.txt. Compare the generated output_rdd.txt with your generated output.txt from Assignment 1 and describe your observations briefly in the submission report (see Part 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81127e07-6e77-40a5-beb4-2784910f36b1",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0488f848-3536-405d-862c-69405ef655b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf\n",
    "import json \n",
    "from operator import add\n",
    "import re\n",
    "from heapq import nlargest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c034031-f89b-4c82-b68d-650a3e8a3901",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SLF4J: Class path contains multiple SLF4J bindings.\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/spark/jars/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: Found binding in [jar:file:/usr/lib/hadoop/lib/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
      "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
      "SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]\n",
      "23/05/29 11:58:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/05/29 11:58:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/05/29 11:59:00 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext.getOrCreate(SparkConf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b895bf5e-bc44-4f23-a75f-c6c4b6c0cb15",
   "metadata": {},
   "source": [
    "### Data loading\n",
    "\n",
    "Note that we could have also loaded the nltk library and downloaded the stopwords. Since this takes time, and efficiency is part of the solution, we have decided to load them directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28084c1e-3aec-43bb-8538-baf035f5a567",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [\"a\" ,\"aa\" ,\"able\" ,\"about\" ,\"above\" ,\"absorbs\" ,\"accord\" ,\"according\" ,\"accordingly\" ,\"across\" ,\"actually\" ,\"after\" ,\"afterwards\" ,\"again\" ,\"against\" ,\"ain\" ,\"album\" ,\"album\" ,\"all\" ,\"allow\" ,\"allows\" ,\"almost\" ,\"alone\" ,\"along\" ,\"already\" ,\"also\" ,\"although\" ,\"always\" ,\"am\" ,\"among\" ,\"amongst\" ,\"an\" ,\"and\" ,\"another\" ,\"any\" ,\"anybody\" ,\"anyhow\" ,\"anyone\" ,\"anything\" ,\"anyway\" ,\"anyways\" ,\"anywhere\" ,\"apart\" ,\"app\" ,\"appear\" ,\"appreciate\" ,\"appropriate\" ,\"are\" ,\"aren\" ,\"around\" ,\"as\" ,\"aside\" ,\"ask\" ,\"asking\" ,\"associated\" ,\"at\" ,\"available\" ,\"away\" ,\"awfully\" ,\"b\" ,\"baby\" ,\"bb\" ,\"be\" ,\"became\" ,\"because\" ,\"become\" ,\"becomes\" ,\"becoming\" ,\"been\" ,\"before\" ,\"beforehand\" ,\"behind\" ,\"being\" ,\"believe\" ,\"below\" ,\"beside\" ,\"besides\" ,\"best\" ,\"better\" ,\"between\" ,\"beyond\" ,\"bibs\" ,\"bike\" ,\"book\" ,\"books\" ,\"both\" ,\"brief\" ,\"bulbs\" ,\"but\" ,\"by\" ,\"c\" ,\"came\" ,\"camera\" ,\"can\" ,\"cannot\" ,\"cant\" ,\"car\" ,\"case\" ,\"cause\" ,\"causes\" ,\"cd\" ,\"certain\" ,\"certainly\" ,\"changes\" ,\"clearly\" ,\"co\" ,\"coffee\" ,\"com\" ,\"come\" ,\"comes\" ,\"concerning\" ,\"consequently\" ,\"consider\" ,\"considering\" ,\"contain\" ,\"containing\" ,\"contains\" ,\"corresponding\" ,\"could\" ,\"couldn\" ,\"course\" ,\"currently\" ,\"d\" ,\"definitely\" ,\"described\" ,\"despite\" ,\"did\" ,\"didn\" ,\"different\" ,\"do\" ,\"does\" ,\"doesn\" ,\"dog\" ,\"dogs\" ,\"doing\" ,\"doll\" ,\"don\" ,\"done\" ,\"down\" ,\"downwards\" ,\"during\" ,\"e\" ,\"each\" ,\"edu\" ,\"eg\" ,\"eight\" ,\"either\" ,\"else\" ,\"elsewhere\" ,\"enough\" ,\"entirely\" ,\"especially\" ,\"et\" ,\"etc\" ,\"even\" ,\"ever\" ,\"every\" ,\"everybody\" ,\"everyone\" ,\"everything\" ,\"everywhere\" ,\"ex\" ,\"exactly\" ,\"example\" ,\"except\" ,\"f\" ,\"far\" ,\"few\" ,\"fifth\" ,\"film\" ,\"first\" ,\"five\" ,\"flavor\" ,\"followed\" ,\"following\" ,\"follows\" ,\"for\" ,\"former\" ,\"formerly\" ,\"forth\" ,\"four\" ,\"from\" ,\"fun\" ,\"further\" ,\"furthermore\" ,\"g\" ,\"game\" ,\"game\" ,\"get\" ,\"gets\" ,\"getting\" ,\"given\" ,\"gives\" ,\"go\" ,\"goes\" ,\"going\" ,\"gone\" ,\"got\" ,\"gotten\" ,\"greetings\" ,\"grill\" ,\"guitar\" ,\"h\" ,\"had\" ,\"hadn\" ,\"hair\" ,\"happens\" ,\"hardly\" ,\"has\" ,\"hasn\" ,\"have\" ,\"haven\" ,\"having\" ,\"he\" ,\"hello\" ,\"help\" ,\"hence\" ,\"her\" ,\"here\" ,\"hereafter\" ,\"hereby\" ,\"herein\" ,\"hereupon\" ,\"hers\" ,\"herself\" ,\"hi\" ,\"him\" ,\"himself\" ,\"his\" ,\"hither\" ,\"hopefully\" ,\"how\" ,\"howbeit\" ,\"however\" ,\"i\" ,\"ie\" ,\"if\" ,\"ignored\" ,\"immediate\" ,\"in\" ,\"inasmuch\" ,\"inc\" ,\"indeed\" ,\"indicate\" ,\"indicated\" ,\"indicates\" ,\"ink\" ,\"inner\" ,\"insofar\" ,\"install\" ,\"instead\" ,\"into\" ,\"inward\" ,\"is\" ,\"isn\" ,\"it\" ,\"its\" ,\"itself\" ,\"j\" ,\"just\" ,\"k\" ,\"keep\" ,\"keeps\" ,\"kept\" ,\"kitchen\" ,\"knife\" ,\"know\" ,\"known\" ,\"knows\" ,\"l\" ,\"lamp\" ,\"laptop\" ,\"last\" ,\"lately\" ,\"later\" ,\"latter\" ,\"latterly\" ,\"least\" ,\"less\" ,\"lest\" ,\"let\" ,\"life\" ,\"like\" ,\"liked\" ,\"likely\" ,\"little\" ,\"ll\" ,\"look\" ,\"looking\" ,\"looks\" ,\"ltd\" ,\"m\" ,\"mainly\" ,\"many\" ,\"may\" ,\"maybe\" ,\"me\" ,\"mean\" ,\"meanwhile\" ,\"merely\" ,\"might\" ,\"mon\" ,\"more\" ,\"moreover\" ,\"most\" ,\"mostly\" ,\"movie\" ,\"mower\" ,\"much\" ,\"must\" ,\"my\" ,\"myself\" ,\"n\" ,\"name\" ,\"namely\" ,\"nd\" ,\"near\" ,\"nearly\" ,\"necessary\" ,\"need\" ,\"needs\" ,\"neither\" ,\"never\" ,\"nevertheless\" ,\"new\" ,\"next\" ,\"nine\" ,\"no\" ,\"nobody\" ,\"non\" ,\"none\" ,\"noone\" ,\"nor\" ,\"normally\" ,\"not\" ,\"nothing\" ,\"novel\" ,\"now\" ,\"nowhere\" ,\"o\" ,\"obviously\" ,\"of\" ,\"off\" ,\"often\" ,\"oh\" ,\"ok\" ,\"okay\" ,\"old\" ,\"on\" ,\"once\" ,\"one\" ,\"ones\" ,\"only\" ,\"onto\" ,\"or\" ,\"other\" ,\"others\" ,\"otherwise\" ,\"ought\" ,\"our\" ,\"ours\" ,\"ourselves\" ,\"out\" ,\"outside\" ,\"over\" ,\"overall\" ,\"own\" ,\"p\" ,\"particular\" ,\"particularly\" ,\"per\" ,\"perhaps\" ,\"phone\" ,\"placed\" ,\"please\" ,\"plus\" ,\"possible\" ,\"presumably\" ,\"printer\" ,\"probably\" ,\"product\" ,\"provides\" ,\"q\" ,\"que\" ,\"quite\" ,\"qv\" ,\"r\" ,\"rather\" ,\"rd\" ,\"re\" ,\"read\" ,\"read\" ,\"really\" ,\"reasonably\" ,\"regarding\" ,\"regardless\" ,\"regards\" ,\"relatively\" ,\"respectively\" ,\"right\" ,\"s\" ,\"said\" ,\"same\" ,\"saw\" ,\"say\" ,\"saying\" ,\"says\" ,\"second\" ,\"secondly\" ,\"see\" ,\"seeing\" ,\"seem\" ,\"seemed\" ,\"seeming\" ,\"seems\" ,\"seen\" ,\"self\" ,\"selves\" ,\"sensible\" ,\"sent\" ,\"serious\" ,\"seriously\" ,\"seven\" ,\"several\" ,\"shall\" ,\"shave\" ,\"she\" ,\"shoes\" ,\"should\" ,\"shouldn\" ,\"since\" ,\"six\" ,\"skin\" ,\"so\" ,\"some\" ,\"somebody\" ,\"somehow\" ,\"someone\" ,\"something\" ,\"sometime\" ,\"sometimes\" ,\"somewhat\" ,\"somewhere\" ,\"song\" ,\"songs\" ,\"soon\" ,\"sorry\" ,\"specified\" ,\"specify\" ,\"specifying\" ,\"still\" ,\"story\" ,\"strings\" ,\"stroller\" ,\"sub\" ,\"such\" ,\"sup\" ,\"sure\" ,\"t\" ,\"take\" ,\"taken\" ,\"taste\" ,\"tell\" ,\"tends\" ,\"th\" ,\"than\" ,\"thank\" ,\"thanks\" ,\"thanx\" ,\"that\" ,\"that\" ,\"thats\" ,\"the\" ,\"their\" ,\"theirs\" ,\"them\" ,\"themselves\" ,\"then\" ,\"thence\" ,\"there\" ,\"there\" ,\"thereafter\" ,\"thereby\" ,\"therefore\" ,\"therein\" ,\"theres\" ,\"thereupon\" ,\"these\" ,\"they\" ,\"think\" ,\"third\" ,\"this\" ,\"thorough\" ,\"thoroughly\" ,\"those\" ,\"though\" ,\"three\" ,\"through\" ,\"throughout\" ,\"thru\" ,\"thus\" ,\"to\" ,\"together\" ,\"too\" ,\"took\" ,\"toward\" ,\"towards\" ,\"toy\" ,\"tried\" ,\"tries\" ,\"truck\" ,\"truly\" ,\"try\" ,\"trying\" ,\"twice\" ,\"two\" ,\"u\" ,\"un\" ,\"under\" ,\"unfortunately\" ,\"unless\" ,\"unlikely\" ,\"until\" ,\"unto\" ,\"up\" ,\"upon\" ,\"us\" ,\"use\" ,\"used\" ,\"useful\" ,\"uses\" ,\"using\" ,\"usually\" ,\"v\" ,\"value\" ,\"various\" ,\"ve\" ,\"very\" ,\"via\" ,\"viz\" ,\"vs\" ,\"want\" ,\"wants\" ,\"was\" ,\"wasn\" ,\"way\" ,\"we\" ,\"wear\" ,\"welcome\" ,\"well\" ,\"went\" ,\"were\" ,\"weren\" ,\"what\" ,\"whatever\" ,\"when\" ,\"whence\" ,\"whenever\" ,\"where\" ,\"whereafter\" ,\"whereas\" ,\"whereby\" ,\"wherein\" ,\"whereupon\" ,\"wherever\" ,\"whether\" ,\"which\" ,\"while\" ,\"whither\" ,\"who\" ,\"whoever\" ,\"whole\" ,\"whom\" ,\"whose\" ,\"why\" ,\"will\" ,\"willing\" ,\"wish\" ,\"with\" ,\"within\" ,\"without\" ,\"won\" ,\"wonder\" ,\"would\" ,\"wouldn\" ,\"x\" ,\"y\" ,\"yes\" ,\"yet\" ,\"you\" ,\"your\" ,\"yours\" ,\"yourself\" ,\"yourselves\" ,\"z\" ,\"zero\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faad777c-87c4-470a-9c0a-595231fbadd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rddfile = sc.textFile(\"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63c2993a-cd3f-4a27-a7bb-409f3dcfb1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"reviewerID\": \"A2VNYWOPJ13AFP\", \"asin\": \"0981850006\", \"reviewerName\": \"Amazon Customer \\\\\"carringt0n\\\\\"\", \"helpful\": [6, 7], \"reviewText\": \"This was a gift for my other husband.  He\\'s making us things from it all the time and we love the food.  Directions are simple, easy to read and interpret, and fun to make.  We all love different kinds of cuisine and Raichlen provides recipes from everywhere along the barbecue trail as he calls it. Get it and just open a page.  Have at it.  You\\'ll love the food and it has provided us with an insight into the culture that produced it. It\\'s all about broadening horizons.  Yum!!\", \"overall\": 5.0, \"summary\": \"Delish\", \"unixReviewTime\": 1259798400, \"reviewTime\": \"12 3, 2009\", \"category\": \"Patio_Lawn_and_Garde\"}'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rddfile.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e584dc07-d633-49ec-a762-57e5f366de68",
   "metadata": {},
   "source": [
    "### Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25b13aa-3237-4b6a-8ad6-0f78b734c26d",
   "metadata": {},
   "source": [
    "##### Total number of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49033a78-b753-40bd-80e9-6e1418478475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "78829"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_docs = rddfile.count()\n",
    "n_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11e44ed-0a2a-49f1-8258-efeb985cd2ff",
   "metadata": {},
   "source": [
    "##### Total number of documents for each category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc2b9e66-68c3-4f63-9917-2ef1d30b1b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Apps_for_Android', 2638),\n",
       " ('Book', 22507),\n",
       " ('Toys_and_Game', 2253),\n",
       " ('Office_Product', 1243),\n",
       " ('Digital_Music', 836),\n",
       " ('Automotive', 1374),\n",
       " ('Beauty', 2023),\n",
       " ('Kindle_Store', 3205),\n",
       " ('Electronic', 7825),\n",
       " ('Movies_and_TV', 4607),\n",
       " ('Tools_and_Home_Improvement', 1926),\n",
       " ('Grocery_and_Gourmet_Food', 1297),\n",
       " ('Patio_Lawn_and_Garde', 994),\n",
       " ('Sports_and_Outdoor', 3269),\n",
       " ('Musical_Instrument', 500),\n",
       " ('CDs_and_Vinyl', 3749),\n",
       " ('Clothing_Shoes_and_Jewelry', 5749),\n",
       " ('Home_and_Kitche', 4254),\n",
       " ('Cell_Phones_and_Accessorie', 3447),\n",
       " ('Pet_Supplie', 1235),\n",
       " ('Baby', 916),\n",
       " ('Health_and_Personal_Care', 2982)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_category = rddfile.map(lambda x: (json.loads(x)['category'],1)).reduceByKey(add)\n",
    "n_category.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8ec1f-e2bb-4dcf-8be8-403063679aa2",
   "metadata": {},
   "source": [
    "##### Tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f59403aa-65f5-40e4-9a07-be6ee85b8d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_category(y):\n",
    "    x = json.loads(y)\n",
    "    catX = x[\"category\"]\n",
    "    revX = x[\"reviewText\"]\n",
    "    #split and distinct:\n",
    "    output = list(set(re.split(\"[\\\\s_\\\\*+-:,;+=/\\\\\\\\\\d%€§&$@#~`\\\"'\\\\.()\\\\t\\\\[\\\\]{}!?]\", revX.casefold()))) \n",
    "    #clear from stopwords and output should not be a single character\n",
    "    output = [i for i in output if i not in stopwords and len(i)>1] \n",
    "    terms = []\n",
    "    for term in output:\n",
    "        terms.append(((term, catX), 1))\n",
    "    return terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e1e5fc-2668-4058-ad39-0f671f2a5030",
   "metadata": {},
   "source": [
    "##### Total number of documents containing a term for each category (this is our A):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8648d8d5-436f-405b-bdbf-6b53d1e39281",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('mic', 'Musical_Instrument'), 24),\n",
       " (('setups', 'Musical_Instrument'), 1),\n",
       " (('tape', 'Musical_Instrument'), 4),\n",
       " (('clip', 'Musical_Instrument'), 5),\n",
       " (('altering', 'Musical_Instrument'), 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_term_category = rddfile.flatMap(term_category).reduceByKey(add)\n",
    "n_term_category.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cfaf53-24ae-44be-9c68-93926f6c81aa",
   "metadata": {},
   "source": [
    "##### Total number of documents containing a term:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdd7cc26-b2a0-4739-90f9-f61437cd00a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('scripture', 115),\n",
       " ('verse', 73),\n",
       " ('tremendously', 58),\n",
       " ('chapter', 849),\n",
       " ('friendly', 310),\n",
       " ('panel', 110),\n",
       " ('lock', 249),\n",
       " ('fix', 424),\n",
       " ('picked', 562),\n",
       " ('free', 1901)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_term = n_term_category.map(lambda x: (x[0][0],x[1])).reduceByKey(add)\n",
    "n_term.take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0980e77d-74d8-46f7-bd77-245919843bcb",
   "metadata": {},
   "source": [
    "##### Join n_term, n_category and n_term_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00e81db8-c3cb-41a4-8b5c-aae2e94d630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('items', (('Musical_Instrument', 5), 691)),\n",
       " ('items', (('CDs_and_Vinyl', 4), 691)),\n",
       " ('items', (('Clothing_Shoes_and_Jewelry', 52), 691)),\n",
       " ('items', (('Home_and_Kitche', 88), 691)),\n",
       " ('items', (('Cell_Phones_and_Accessorie', 35), 691))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_term_category_key = n_term_category.map(lambda x: (x[0][0],(x[0][1],x[1])))\n",
    "first_join = n_term_category_key.join(n_term)\n",
    "first_join.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58be3de6-8a0f-4d07-993c-c5db25b38a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Toys_and_Game', (((44, 'items'), 691), 2253)),\n",
       " ('Toys_and_Game', (((6, 'length'), 637), 2253)),\n",
       " ('Toys_and_Game', (((34, 'kit'), 365), 2253)),\n",
       " ('Toys_and_Game', (((28, 'quick'), 1432), 2253)),\n",
       " ('Toys_and_Game', (((315, 'good'), 16025), 2253))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_join_key = first_join.map(lambda x: (x[1][0][0],((x[1][0][1],x[0]),x[1][1])))\n",
    "second_join = first_join_key.join(n_category)\n",
    "second_join.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70373ce0-2931-4120-9c2e-5742e83b5718",
   "metadata": {},
   "source": [
    "##### Calculating chisquared values\n",
    "\n",
    "Reminder: input is in the form of (category, (((n_term_category, term), n_term), n_category_category))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "292fdc81-7076-4016-89dd-e9eea54b11cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chisquare(thejoinedattributes):\n",
    "    \n",
    "    category = thejoinedattributes[0]\n",
    "    term = thejoinedattributes[1][0][0][1]\n",
    "    \n",
    "    n_term_category = thejoinedattributes[1][0][0][0]\n",
    "    n_term = thejoinedattributes[1][0][1]\n",
    "    n_category_category = thejoinedattributes[1][1]\n",
    "    \n",
    "    A = n_term_category\n",
    "    B = n_term - A\n",
    "    C = n_category_category - A\n",
    "    D = n_docs - n_category_category - B\n",
    "    \n",
    "    AD = A*D\n",
    "    BC = B*C\n",
    "    AB = A+B\n",
    "    AC = A+C\n",
    "    BD = B+D\n",
    "    CD = C+D\n",
    "\n",
    "    nominator = n_docs*(AD-BC)**2\n",
    "    denominator = AB*AC*BD*CD\n",
    "    chisquare = nominator/denominator\n",
    "    \n",
    "    return (category, (term, chisquare))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a980ba5-ef58-4d2a-82dd-72e6152064f0",
   "metadata": {},
   "source": [
    "##### Applying the calculations\n",
    "\n",
    "Reminder: we need to calculate all chisquares, group them by category in descending order, take the top 75 for each category, append them one after the other and sort everything by the category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "226a80f0-5ba6-4cff-b85d-deccf818d360",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "chisquared = second_join.map(chisquare).sortBy(lambda x: x[1][1], ascending=False).groupBy(lambda x: x[0]).flatMap(lambda g: nlargest(75, g[1], key=lambda x: x[1][1])).reduceByKey(add).sortByKey(True).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a8ff77-ee25-474c-a9d4-fc088b29121d",
   "metadata": {},
   "source": [
    "### Saving to file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5675d71-3296-46ab-bb91-1c9f17f9622e",
   "metadata": {},
   "source": [
    "##### Generating the dictionary for the last line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a10be8b-d132-4b36-9c5a-ddf79d36cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "lastline = set()\n",
    "\n",
    "for i in chisquared:\n",
    "    term_chi_array = i[1]\n",
    "    for u in term_chi_array:\n",
    "        if type(u) == str:\n",
    "            lastline.add(u)\n",
    "\n",
    "last_output = sorted(list(lastline))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1845c43-433f-49d1-9a60-3f5cc4c8427e",
   "metadata": {},
   "source": [
    "##### Writing the chisquare results and the last line dictionary to output_rdd.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d21f13d2-2b45-46a6-a418-ddac6262e908",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_rdd.txt', 'w') as f:\n",
    "    for i in chisquared:\n",
    "        f.write(re.sub(\"\\(|\\)|,|'|\\[|\\]\", \"\", str(i)))\n",
    "        f.write('\\n')    \n",
    "    f.write(str(re.sub(\",|'|[0-9]|\\[|\\]|\\.\",\"\", str(last_output))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfa0752-0f9e-4f47-aa09-f91cbe0086ba",
   "metadata": {},
   "source": [
    "## Part 2) Datasets/DataFrames: Spark ML and Pipelines\n",
    "\n",
    "Convert the review texts to a classic vector space representation with TFIDF-weighted features based on the Spark DataFrame/Dataset API by building a transformation pipeline. The primary goal of this part is the preparation of the pipeline for Part 3 (see below). Note: although parts of this pipeline will be very similar to Assignment 1 or Part 1 above, do not expect to obtain identical results or have access to all intermediate outputs to compare the individual steps.\n",
    "\n",
    "Use built-in functions for tokenization to unigrams at whitespaces, tabs, digits, and the delimiter characters ()[]{}.!?,;:+=-_\"'`~#@&*%€$§\\/, casefolding, stopword removal, TF-IDF calculation, and chi square selection ) (using 2000 top terms overall). Write the terms selected this way to a file output_ds.txt and compare them with the terms selected in Assignment 1. Describe your observations briefly in the submission report (see Part 3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41086285-77b1-4b91-9986-51e9e05f927b",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43008635-2bb1-42eb-80b0-5b0eb0dc93c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StopWordsRemover, CountVectorizer, IDF, Tokenizer, RegexTokenizer, StringIndexer, ChiSqSelector\n",
    "from pyspark.ml import Pipeline\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3510014c-230f-4632-a3c8-cc2bb25463a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382f5530-8ba4-4dd7-a01f-eb81c41c6104",
   "metadata": {},
   "source": [
    "### Building a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0468add0-9ed5-4963-990d-2db8f58b6229",
   "metadata": {},
   "source": [
    "##### Define the parts of the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8985e775-6a47-4073-a2a2-a7f939749bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read in the reviews json file from the hdfs directory and create a dataframe with the columns category and reviewText\n",
    "textDF = spark.read.json(\"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\").createOrReplaceTempView(\"review\")\n",
    "reviewTextDF = spark.sql(\"SELECT category,reviewText FROM review\")\n",
    "\n",
    "# Case folding and tokenization using whitespaces, tabs, digits\n",
    "# and the characters ()[]{}.!?,;:+=-_\"'`~#@&*%€$§\\/ as delimiters\n",
    "regexTokenizer = RegexTokenizer().setInputCol(\"reviewText\").setOutputCol(\"terms\").setPattern(\"[\\\\s_\\\\*+-:,;+=/\\\\\\\\%€[0-9]$&§@#~`\\\"'\\\\.()\\\\t\\\\[\\\\]{}!?]\")\n",
    "\n",
    "# stopword removal\n",
    "remover = StopWordsRemover().setInputCol(\"terms\").setOutputCol(\"filteredTerms\")\n",
    "\n",
    "# limit the vocabulary size to improve runtime.setMinDF(5) // min document frequency\n",
    "countVec = CountVectorizer().setInputCol(\"filteredTerms\").setOutputCol(\"features\").setVocabSize(20000) \n",
    "\n",
    "# TF-IDF calculation\n",
    "idf = IDF().setInputCol(\"features\").setOutputCol(\"weightedFeatures\")\n",
    "\n",
    "# encode the categories as labels\n",
    "encoder = StringIndexer().setInputCol(\"category\").setOutputCol(\"categoryIndex\")\n",
    "\n",
    "# chi square selection (using 2000 top terms overall)\n",
    "selector = ChiSqSelector().setNumTopFeatures(2000).setFeaturesCol(\"weightedFeatures\").setLabelCol(\"categoryIndex\").setOutputCol(\"selectedFeatures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c57e91f-2a73-4235-9dde-26f2124dba8c",
   "metadata": {},
   "source": [
    "##### Stack together to a pipeline, fit and transform the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "884415f8-c04f-4534-9e1f-eaece56fb9a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "|            category|          reviewText|               terms|       filteredTerms|            features|    weightedFeatures|categoryIndex|    selectedFeatures|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...|[this, was, a, gi...|[gift, husband, m...|(20000,[5,7,8,9,2...|(20000,[5,7,8,9,2...|         18.0|(2000,[5,7,8,9,27...|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...|[this, is, a, ver...|[nice, spreader, ...|(20000,[2,4,8,9,1...|(20000,[2,4,8,9,1...|         18.0|(2000,[2,4,8,9,16...|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...|[the, metal, base...|[metal, base, hos...|(20000,[6,20,21,3...|(20000,[6,20,21,3...|         18.0|(2000,[6,19,20,37...|\n",
      "|Patio_Lawn_and_Garde|For the most part...|[for, the, most, ...|[part, works, pre...|(20000,[1,3,4,8,1...|(20000,[1,3,4,8,1...|         18.0|(2000,[1,3,4,8,14...|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...|[this, hose, is, ...|[hose, supposed, ...|(20000,[1,37,42,4...|(20000,[1,37,42,4...|         18.0|(2000,[1,36,41,43...|\n",
      "|Patio_Lawn_and_Garde|This tool works v...|[this, tool, work...|[tool, works, wel...|(20000,[2,6,8,13,...|(20000,[2,6,8,13,...|         18.0|(2000,[2,6,8,13,1...|\n",
      "|Patio_Lawn_and_Garde|This product is a...|[this, product, i...|[product, typical...|(20000,[6,14,19,5...|(20000,[6,14,19,5...|         18.0|(2000,[6,14,18,49...|\n",
      "|Patio_Lawn_and_Garde|I was excited to ...|[i, was, excited,...|[excited, ditch, ...|(20000,[25,27,33,...|(20000,[25,27,33,...|         18.0|(2000,[24,26,32,5...|\n",
      "|Patio_Lawn_and_Garde|I purchased the L...|[i, purchased, th...|[purchased, leaf,...|(20000,[8,11,21,2...|(20000,[8,11,21,2...|         18.0|(2000,[8,11,20,23...|\n",
      "|Patio_Lawn_and_Garde|Never used a manu...|[never, used, a, ...|[never, used, man...|(20000,[16,25,31,...|(20000,[16,25,31,...|         18.0|(2000,[16,24,30,4...|\n",
      "|Patio_Lawn_and_Garde|Good price. Good ...|[good, price, goo...|[good, price, goo...|(20000,[1,4,5,17,...|(20000,[1,4,5,17,...|         18.0|(2000,[1,4,5,17,1...|\n",
      "|Patio_Lawn_and_Garde|I have owned the ...|[i, have, owned, ...|[owned, flowtron,...|(20000,[9,19,24,4...|(20000,[9,19,24,4...|         18.0|(2000,[9,18,23,45...|\n",
      "|Patio_Lawn_and_Garde|I had \"won\" a sim...|[i, had, won, a, ...|[won, similar, pr...|(20000,[4,11,14,1...|(20000,[4,11,14,1...|         18.0|(2000,[4,11,14,18...|\n",
      "|Patio_Lawn_and_Garde|The birds ate all...|[the, birds, ate,...|[birds, ate, blue...|(20000,[3,18,76,1...|(20000,[3,18,76,1...|         18.0|(2000,[3,69,94,98...|\n",
      "|Patio_Lawn_and_Garde|Bought last summe...|[bought, last, su...|[bought, last, su...|(20000,[1,2,8,9,1...|(20000,[1,2,8,9,1...|         18.0|(2000,[1,2,8,9,13...|\n",
      "|Patio_Lawn_and_Garde|I knew I had a mo...|[i, knew, i, had,...|[knew, mouse, bas...|(20000,[1,6,14,16...|(20000,[1,6,14,16...|         18.0|(2000,[1,6,14,16,...|\n",
      "|Patio_Lawn_and_Garde|I was a little wo...|[i, was, a, littl...|[little, worried,...|(20000,[1,4,6,17,...|(20000,[1,4,6,17,...|         18.0|(2000,[1,4,6,17,2...|\n",
      "|Patio_Lawn_and_Garde|I have used this ...|[i, have, used, t...|[used, brand, lon...|(20000,[7,8,46,61...|(20000,[7,8,46,61...|         18.0|(2000,[7,8,45,156...|\n",
      "|Patio_Lawn_and_Garde|I actually do not...|[i, actually, do,...|[actually, curren...|(20000,[1,14,16,1...|(20000,[1,14,16,1...|         18.0|(2000,[1,14,16,18...|\n",
      "|Patio_Lawn_and_Garde|Just what I  expe...|[just, what, i, e...|[expected, works,...|(20000,[2,14,50,8...|(20000,[2,14,50,8...|         18.0|(2000,[2,14,49,77...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline().setStages([regexTokenizer, remover, countVec, idf, encoder, selector])\n",
    "\n",
    "# fit the pipeline to training documents.\n",
    "model = pipeline.fit(reviewTextDF)\n",
    "\n",
    "finalDF = model.transform(reviewTextDF)\n",
    "finalDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02348a98-acae-4ef1-aa20-732477eb6f28",
   "metadata": {},
   "source": [
    "### Get the distinct terms and sort them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a55eb41-5959-4ee2-96dc-c85a44131a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selectedFeatures = model.stages[5].selectedFeatures\n",
    "vocab = model.stages[2].vocabulary\n",
    "\n",
    "output = set()\n",
    "for i in selectedFeatures:\n",
    "    output.add(vocab[i])\n",
    "\n",
    "sorted_output = sorted(list(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed7d78b7-3a96-42b7-b09a-33e81ace0f23",
   "metadata": {},
   "source": [
    "### Save to output_ds.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f40bfda-bd31-497b-9ad4-4a3b5cec1836",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output_ds.txt', 'w') as f:  \n",
    "    f.write(str(re.sub(\",|'|[0-9]|\\[|\\]|\\.\",\"\", str(sorted_output))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0322ad32-2901-46a6-bef6-dc2dc7fc373a",
   "metadata": {},
   "source": [
    "## Part 3) Text Classification\n",
    "\n",
    "In this part, you will train a text classifier from the features extracted in Part 2. The goal is to learn a model that can predict the product category from a review's text.\n",
    "\n",
    "To this end, extend the pipeline from Part 2 such that a Support Vector Machine classifier is trained. Since we are dealing with multi-class problems, make sure to put a strategy in place that allows binary classifiers to be applicable. Apply vector length normalization before feeding the feature vectors into the classifier (use Normalizer with L2 norm).\n",
    "\n",
    "Follow best practices for machine learning experiment design and investigate the effects of parameter settings using the functions provided by Spark:\n",
    "\n",
    "Split the review data into training, validation, and test set.\n",
    "\n",
    "Make experiments reproducible.\n",
    "\n",
    "Use a grid search for parameter optimization:\n",
    "\n",
    "Compare chi square overall top 2000 filtered features with another, heavier filtering with much less dimensionality (see Spark ML documentation for options).\n",
    "\n",
    "Compare different SVM settings by varying the regularization parameter (choose 3 different values), standardization of training features (2 values), and maximum number of iterations (2 values).\n",
    "\n",
    "Use the MulticlassClassificationEvaluator to estimate performance of your trained classifiers on the test set, using F1 measure as criterion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216e849b-619f-41e5-9e32-c29c4ace76e2",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3ba19726-8891-465b-b7f0-4fa84255f2b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'MulticlassMetrics' from 'pyspark.ml.evaluation' (/usr/lib/spark/python/pyspark/ml/evaluation.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearSVC\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclassification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression, OneVsRest\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MulticlassClassificationEvaluator, MulticlassMetrics\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'MulticlassMetrics' from 'pyspark.ml.evaluation' (/usr/lib/spark/python/pyspark/ml/evaluation.py)"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Matrices\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22723fce-1e30-4251-80d8-f99ac5db121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Dataframe with the relevant columns\n",
    "mod = finalDF.select(\"categoryIndex\",\"selectedFeatures\").toDF(\"label\",\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a4e2c8-effe-405c-952c-d1a23aaecae3",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90c7074b-c8f1-4047-aad5-467f2a34db98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^2 norm\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|        normFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "| 18.0|(2000,[5,7,8,9,27...|(2000,[5,7,8,9,27...|\n",
      "| 18.0|(2000,[2,4,8,9,16...|(2000,[2,4,8,9,16...|\n",
      "| 18.0|(2000,[6,19,20,37...|(2000,[6,19,20,37...|\n",
      "| 18.0|(2000,[1,3,4,8,14...|(2000,[1,3,4,8,14...|\n",
      "| 18.0|(2000,[1,36,41,43...|(2000,[1,36,41,43...|\n",
      "| 18.0|(2000,[2,6,8,13,1...|(2000,[2,6,8,13,1...|\n",
      "| 18.0|(2000,[6,14,18,49...|(2000,[6,14,18,49...|\n",
      "| 18.0|(2000,[24,26,32,5...|(2000,[24,26,32,5...|\n",
      "| 18.0|(2000,[8,11,20,23...|(2000,[8,11,20,23...|\n",
      "| 18.0|(2000,[16,24,30,4...|(2000,[16,24,30,4...|\n",
      "| 18.0|(2000,[1,4,5,17,1...|(2000,[1,4,5,17,1...|\n",
      "| 18.0|(2000,[9,18,23,45...|(2000,[9,18,23,45...|\n",
      "| 18.0|(2000,[4,11,14,18...|(2000,[4,11,14,18...|\n",
      "| 18.0|(2000,[3,69,94,98...|(2000,[3,69,94,98...|\n",
      "| 18.0|(2000,[1,2,8,9,13...|(2000,[1,2,8,9,13...|\n",
      "| 18.0|(2000,[1,6,14,16,...|(2000,[1,6,14,16,...|\n",
      "| 18.0|(2000,[1,4,6,17,2...|(2000,[1,4,6,17,2...|\n",
      "| 18.0|(2000,[7,8,45,156...|(2000,[7,8,45,156...|\n",
      "| 18.0|(2000,[1,14,16,18...|(2000,[1,14,16,18...|\n",
      "| 18.0|(2000,[2,14,49,77...|(2000,[2,14,49,77...|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# apply vector length normalization before feeding the feature vectors into the classifier (use Normalizer with L2 norm).\n",
    "normalizer = Normalizer().setInputCol(\"features\").setOutputCol(\"normFeatures\").setP(2.0)\n",
    "l2NormData = normalizer.transform(mod)\n",
    "\n",
    "print(\"Normalized using L^2 norm\")\n",
    "l2NormData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d16f0a95-ef95-4f90-8741-a711d60fda3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/spark/python/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mod = l2NormData.select(\"label\",\"normFeatures\").toDF(\"label\",\"features\")\n",
    "\n",
    "# convert DataFrame columns and save as libsvm.\n",
    "convertedVecDF = MLUtils.convertVectorColumnsToML(mod)\n",
    "convertedVecDF.write.mode(\"overwrite\").format(\"libsvm\").save(\"data/Part3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8b9415b-74e7-4b13-9418-ec356683e455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/29 12:01:20 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load data file.\n",
    "inputData = spark.read.format(\"libsvm\").load(\"data/Part3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "729ff207-fa0f-4530-af7a-6a27798f5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training, validation and test set.\n",
    "# generate the train/test split.\n",
    "# make experiments reproducible.\n",
    "(train_validation,test) = inputData.randomSplit([0.8, 0.2], seed = 42)\n",
    "(train, validation) = train_validation.randomSplit([0.8, 0.2], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42637913-448e-4e34-87c9-5d47bc2acb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Support Vector Machine classifier.\n",
    "lsvc = LinearSVC(maxIter=10, regParam=0.1)\n",
    "\n",
    "# strategy that allows binary classifiers to be applicable: instantiate the One Vs Rest Classifier.\n",
    "OVRclassifier = OneVsRest(classifier=lsvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a7d9a51-7a8a-4586-96da-a619f55717ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/29 12:01:27 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/05/29 12:01:27 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "23/05/29 12:01:27 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "23/05/29 12:01:27 WARN BLAS: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# train the multiclass model.\n",
    "OVRModel = OVRclassifier.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9df025-9cb1-474b-bc4c-c87e11c5b1ec",
   "metadata": {},
   "source": [
    "## Parameters for Test Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "739b0095-a89f-47d2-8221-6f1ba12142e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LinearSVC_26a8d959c6df', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LinearSVC_26a8d959c6df', name='threshold', doc='The threshold in binary classification applied to the linear model prediction.  This threshold can be any real number, where Inf will make all predictions 0.0 and -Inf will make all predictions 1.0.'): 0.0,\n",
       " Param(parent='LinearSVC_26a8d959c6df', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LinearSVC_26a8d959c6df', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LinearSVC_26a8d959c6df', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LinearSVC_26a8d959c6df', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LinearSVC_26a8d959c6df', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='LinearSVC_26a8d959c6df', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent='LinearSVC_26a8d959c6df', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LinearSVC_26a8d959c6df', name='maxIter', doc='max number of iterations (>= 0).'): 10,\n",
       " Param(parent='LinearSVC_26a8d959c6df', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       " Param(parent='LinearSVC_26a8d959c6df', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OVRModel.getClassifier().extractParamMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "454b0e0f-a0aa-46f2-b6e9-61f559e98ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(label=0.0, features=SparseVector(2000, {}), rawPrediction=DenseVector([-0.8415, -1.2584, -1.4338, -1.2125, -1.6101, -2.2794, -1.3386, -1.2592, -1.1368, -1.3162, -1.2537, -1.5464, -1.494, -1.3791, -1.5059, -1.6248, -1.5896, -1.742, -1.6181, -1.8916, -1.2212, -2.0833]), prediction=0.0)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# score the model on test data.\n",
    "predictions = OVRModel.transform(validation)\n",
    "predictions.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "866d3982-d947-4491-af60-01f94c36eeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the MulticlassClassificationEvaluator to estimate performance of the trained classifiers on the test set.\n",
    "# use F1 measure as criterion.\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b05daf5-da72-42b2-92db-79f372fa3207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# compute the classification error on test data.\n",
    "accuracy = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c157898c-ea93-4a57-bd08-64d8e8f65cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = 0.315052\n"
     ]
    }
   ],
   "source": [
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d726e7e3-2c4a-4ccb-80af-82a0e994e74f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SVM Gridsearch\n",
    "- Compare chi square overall top 2000 filtered features with another, heavier filtering with much less dimensionality (see Spark ML documentation for options).\n",
    "- Compare different SVM settings by varying the regularization parameter (choose 3 different values), standardization of training features (2 values), and maximum number of iterations (2 values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "208a8780-5b9f-4eab-98ca-c0e1bd63e8aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "# train and validation data\n",
    "train_validation\n",
    "# test data\n",
    "test\n",
    "\n",
    "# train a Support Vector Machine classifier.\n",
    "lsvc = LinearSVC()\n",
    "\n",
    "# strategy that allows binary classifiers to be applicable: instantiate the One Vs Rest Classifier.\n",
    "OVRclassifier = OneVsRest(classifier=lsvc)\n",
    "mcevaluator = MulticlassClassificationEvaluator(metricName=\"f1\")\n",
    "\n",
    "# We use a ParamGridBuilder to construct a grid of parameters to search over.\n",
    "# TrainValidationSplit will try all combinations of values and determine best model using\n",
    "# the evaluator.\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lsvc.regParam, [0.2, 0.1, 0.01]) \\\n",
    "    .addGrid(lsvc.standardization, [False, True])\\\n",
    "    .addGrid(lsvc.maxIter, [5, 10])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=OVRclassifier,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=mcevaluator,\n",
    "                           # 80% of the data will be used for training, 20% for validation.\n",
    "                           trainRatio=0.8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f91aa6-655f-4311-b984-7b3b017c6784",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Generated Parameter Grid - Those get evaluated with F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5cd15b4-7f71-4cb0-bcb5-d2892934471b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.2,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): False,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 5},\n",
       " {Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.2,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): False,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 10},\n",
       " {Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.2,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 5},\n",
       " {Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.2,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 10},\n",
       " {Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): False,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 5},\n",
       " {Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): False,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 10},\n",
       " {Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 5},\n",
       " {Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.1,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 10},\n",
       " {Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): False,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 5},\n",
       " {Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): False,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 10},\n",
       " {Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 5},\n",
       " {Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       "  Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 10}]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paramGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e031b809-2d57-44fe-8aba-8b4ee0dcd796",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model2000 = tvs.fit(train_validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "62a2b77f-b79a-49d0-8281-a52fb6da4e0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model2000.save(\"data/gridsearch-model-2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50c318-656e-48ce-9554-2e9cd5b60e87",
   "metadata": {},
   "source": [
    "## Get chosen Parameters from Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "98d2cd55-f360-47f7-b1d6-b40931af10b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2000.bestModel.getClassifier().getMaxIter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2aafda4-f63c-4c21-a51e-1f475e6a551d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2000.bestModel.getClassifier().getStandardization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d300829a-b3f4-4f10-857e-7c05c9470b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2000.bestModel.getClassifier().getRegParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "85f90067-dfe7-495b-a3af-922e72b76a31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{Param(parent='LinearSVC_3813097dd5cf', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
       " Param(parent='LinearSVC_3813097dd5cf', name='featuresCol', doc='features column name.'): 'features',\n",
       " Param(parent='LinearSVC_3813097dd5cf', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
       " Param(parent='LinearSVC_3813097dd5cf', name='labelCol', doc='label column name.'): 'label',\n",
       " Param(parent='LinearSVC_3813097dd5cf', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
       " Param(parent='LinearSVC_3813097dd5cf', name='maxIter', doc='max number of iterations (>= 0).'): 10,\n",
       " Param(parent='LinearSVC_3813097dd5cf', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
       " Param(parent='LinearSVC_3813097dd5cf', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction',\n",
       " Param(parent='LinearSVC_3813097dd5cf', name='regParam', doc='regularization parameter (>= 0).'): 0.01,\n",
       " Param(parent='LinearSVC_3813097dd5cf', name='standardization', doc='whether to standardize the training features before fitting the model.'): True,\n",
       " Param(parent='LinearSVC_3813097dd5cf', name='threshold', doc='The threshold in binary classification applied to the linear model prediction.  This threshold can be any real number, where Inf will make all predictions 0.0 and -Inf will make all predictions 1.0.'): 0.0,\n",
       " Param(parent='LinearSVC_3813097dd5cf', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2000.bestModel.getClassifier().extractParamMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e3cbc0-220c-4a67-a955-4ff9f39a6b43",
   "metadata": {},
   "source": [
    "## Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4ee4b70c-6837-42a1-9bc5-824834ce2329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "mypred2000 = model2000.transform(test)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceed340b-f040-4f3f-80b2-628dcc252f9c",
   "metadata": {},
   "source": [
    "## These are the Results for 2000 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e4dfbf8e-40d8-4d32-ab40-48452ac81150",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6892236976506639"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MulticlassClassificationEvaluator(metricName=\"accuracy\").evaluate(mypred2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "11aeb2cb-5c93-4c1e-ae77-30547be740bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6597480289890458"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MulticlassClassificationEvaluator(metricName=\"f1\").evaluate(mypred2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "25d04baa-2cfc-48b3-be8b-d8f4b58a1fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.31077630234933606"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# again 0.31 error\n",
    "mypred2000.filter(mypred2000.prediction != mypred2000.label).count() / mypred2000.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252b16e5-7d8f-4b46-87ef-da6539704bb7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Less dimensionalty - 50 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a53c5052-ed5f-42c2-b85a-82d5a57cbb52",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "|            category|          reviewText|               terms|       filteredTerms|            features|    weightedFeatures|categoryIndex|    selectedFeatures|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "|Patio_Lawn_and_Garde|This was a gift f...|[this, was, a, gi...|[gift, husband, m...|(20000,[5,7,8,9,2...|(20000,[5,7,8,9,2...|         18.0|(50,[5,7,8,9,27,3...|\n",
      "|Patio_Lawn_and_Garde|This is a very ni...|[this, is, a, ver...|[nice, spreader, ...|(20000,[2,4,8,9,1...|(20000,[2,4,8,9,1...|         18.0|(50,[2,4,8,9,16,1...|\n",
      "|Patio_Lawn_and_Garde|The metal base wi...|[the, metal, base...|[metal, base, hos...|(20000,[6,20,21,3...|(20000,[6,20,21,3...|         18.0|(50,[6,19,20,37],...|\n",
      "|Patio_Lawn_and_Garde|For the most part...|[for, the, most, ...|[part, works, pre...|(20000,[1,3,4,8,1...|(20000,[1,3,4,8,1...|         18.0|(50,[1,3,4,8,14,1...|\n",
      "|Patio_Lawn_and_Garde|This hose is supp...|[this, hose, is, ...|[hose, supposed, ...|(20000,[1,37,42,4...|(20000,[1,37,42,4...|         18.0|(50,[1,36,41,43],...|\n",
      "|Patio_Lawn_and_Garde|This tool works v...|[this, tool, work...|[tool, works, wel...|(20000,[2,6,8,13,...|(20000,[2,6,8,13,...|         18.0|(50,[2,6,8,13,14,...|\n",
      "|Patio_Lawn_and_Garde|This product is a...|[this, product, i...|[product, typical...|(20000,[6,14,19,5...|(20000,[6,14,19,5...|         18.0|(50,[6,14,18,49],...|\n",
      "|Patio_Lawn_and_Garde|I was excited to ...|[i, was, excited,...|[excited, ditch, ...|(20000,[25,27,33,...|(20000,[25,27,33,...|         18.0|(50,[24,26,32],[2...|\n",
      "|Patio_Lawn_and_Garde|I purchased the L...|[i, purchased, th...|[purchased, leaf,...|(20000,[8,11,21,2...|(20000,[8,11,21,2...|         18.0|(50,[8,11,20,23,2...|\n",
      "|Patio_Lawn_and_Garde|Never used a manu...|[never, used, a, ...|[never, used, man...|(20000,[16,25,31,...|(20000,[16,25,31,...|         18.0|(50,[16,24,30,45]...|\n",
      "|Patio_Lawn_and_Garde|Good price. Good ...|[good, price, goo...|[good, price, goo...|(20000,[1,4,5,17,...|(20000,[1,4,5,17,...|         18.0|(50,[1,4,5,17,18,...|\n",
      "|Patio_Lawn_and_Garde|I have owned the ...|[i, have, owned, ...|[owned, flowtron,...|(20000,[9,19,24,4...|(20000,[9,19,24,4...|         18.0|(50,[9,18,23,45,4...|\n",
      "|Patio_Lawn_and_Garde|I had \"won\" a sim...|[i, had, won, a, ...|[won, similar, pr...|(20000,[4,11,14,1...|(20000,[4,11,14,1...|         18.0|(50,[4,11,14,18,2...|\n",
      "|Patio_Lawn_and_Garde|The birds ate all...|[the, birds, ate,...|[birds, ate, blue...|(20000,[3,18,76,1...|(20000,[3,18,76,1...|         18.0|(50,[3],[1.551286...|\n",
      "|Patio_Lawn_and_Garde|Bought last summe...|[bought, last, su...|[bought, last, su...|(20000,[1,2,8,9,1...|(20000,[1,2,8,9,1...|         18.0|(50,[1,2,8,9,13,1...|\n",
      "|Patio_Lawn_and_Garde|I knew I had a mo...|[i, knew, i, had,...|[knew, mouse, bas...|(20000,[1,6,14,16...|(20000,[1,6,14,16...|         18.0|(50,[1,6,14,16,17...|\n",
      "|Patio_Lawn_and_Garde|I was a little wo...|[i, was, a, littl...|[little, worried,...|(20000,[1,4,6,17,...|(20000,[1,4,6,17,...|         18.0|(50,[1,4,6,17,22,...|\n",
      "|Patio_Lawn_and_Garde|I have used this ...|[i, have, used, t...|[used, brand, lon...|(20000,[7,8,46,61...|(20000,[7,8,46,61...|         18.0|(50,[7,8,45],[1.8...|\n",
      "|Patio_Lawn_and_Garde|I actually do not...|[i, actually, do,...|[actually, curren...|(20000,[1,14,16,1...|(20000,[1,14,16,1...|         18.0|(50,[1,14,16,18,2...|\n",
      "|Patio_Lawn_and_Garde|Just what I  expe...|[just, what, i, e...|[expected, works,...|(20000,[2,14,50,8...|(20000,[2,14,50,8...|         18.0|(50,[2,14,49],[1....|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "selector50 = ChiSqSelector().setNumTopFeatures(50).setFeaturesCol(\"weightedFeatures\").setLabelCol(\"categoryIndex\").setOutputCol(\"selectedFeatures\")\n",
    "\n",
    "pipeline50 = Pipeline().setStages([regexTokenizer, remover, countVec, idf, encoder, selector50])\n",
    "\n",
    "# fit the pipeline to training documents.\n",
    "model50 = pipeline50.fit(reviewTextDF)\n",
    "\n",
    "finalDF50 = model50.transform(reviewTextDF)\n",
    "finalDF50.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2a6475b-8d35-46bd-94cc-d39510549d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized using L^2 norm\n",
      "+-----+--------------------+--------------------+\n",
      "|label|            features|        normFeatures|\n",
      "+-----+--------------------+--------------------+\n",
      "| 18.0|(50,[5,7,8,9,27,3...|(50,[5,7,8,9,27,3...|\n",
      "| 18.0|(50,[2,4,8,9,16,1...|(50,[2,4,8,9,16,1...|\n",
      "| 18.0|(50,[6,19,20,37],...|(50,[6,19,20,37],...|\n",
      "| 18.0|(50,[1,3,4,8,14,1...|(50,[1,3,4,8,14,1...|\n",
      "| 18.0|(50,[1,36,41,43],...|(50,[1,36,41,43],...|\n",
      "| 18.0|(50,[2,6,8,13,14,...|(50,[2,6,8,13,14,...|\n",
      "| 18.0|(50,[6,14,18,49],...|(50,[6,14,18,49],...|\n",
      "| 18.0|(50,[24,26,32],[2...|(50,[24,26,32],[0...|\n",
      "| 18.0|(50,[8,11,20,23,2...|(50,[8,11,20,23,2...|\n",
      "| 18.0|(50,[16,24,30,45]...|(50,[16,24,30,45]...|\n",
      "| 18.0|(50,[1,4,5,17,18,...|(50,[1,4,5,17,18,...|\n",
      "| 18.0|(50,[9,18,23,45,4...|(50,[9,18,23,45,4...|\n",
      "| 18.0|(50,[4,11,14,18,2...|(50,[4,11,14,18,2...|\n",
      "| 18.0|(50,[3],[1.551286...|      (50,[3],[1.0])|\n",
      "| 18.0|(50,[1,2,8,9,13,1...|(50,[1,2,8,9,13,1...|\n",
      "| 18.0|(50,[1,6,14,16,17...|(50,[1,6,14,16,17...|\n",
      "| 18.0|(50,[1,4,6,17,22,...|(50,[1,4,6,17,22,...|\n",
      "| 18.0|(50,[7,8,45],[1.8...|(50,[7,8,45],[0.4...|\n",
      "| 18.0|(50,[1,14,16,18,2...|(50,[1,14,16,18,2...|\n",
      "| 18.0|(50,[2,14,49],[1....|(50,[2,14,49],[0....|\n",
      "+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mod50 = finalDF50.select(\"categoryIndex\",\"selectedFeatures\").toDF(\"label\",\"features\")\n",
    "# apply vector length normalization before feeding the feature vectors into the classifier (use Normalizer with L2 norm).\n",
    "normalizer = Normalizer().setInputCol(\"features\").setOutputCol(\"normFeatures\").setP(2.0)\n",
    "l2NormData50 = normalizer.transform(mod50)\n",
    "\n",
    "print(\"Normalized using L^2 norm\")\n",
    "l2NormData50.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88abd38e-d053-4669-a7b9-0659af29844b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mod50 = l2NormData50.select(\"label\",\"normFeatures\").toDF(\"label\",\"features\")\n",
    "\n",
    "# convert DataFrame columns and save as libsvm.\n",
    "convertedVecDF50 = MLUtils.convertVectorColumnsToML(mod50)\n",
    "convertedVecDF50.write.mode(\"overwrite\").format(\"libsvm\").save(\"data/Part3-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48418600-bceb-4112-84da-1080e05d9e4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SVM with 50 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "69bede59-bc39-43f6-8eb0-26a508ae81fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/29 12:11:29 WARN LibSVMFileFormat: 'numFeatures' option not specified, determining the number of features by going though the input. If you know the number in advance, please specify it via 'numFeatures' option to avoid the extra scan.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# load data file. data/Part3-200 is also available\n",
    "inputData50 = spark.read.format(\"libsvm\").load(\"data/Part3-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b3e4a921-d28b-4065-a0f3-b018fd746e5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the data into training, validation and test set.\n",
    "# generate the train/test split.\n",
    "# make experiments reproducible.\n",
    "(train_validation50,test50) = inputData50.randomSplit([0.8, 0.2], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8980d503-a3fa-4526-bc2f-efb9cfbcdb95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/29 12:12:27 ERROR OWLQN: Failure! Resetting history: breeze.optimize.NaNHistory: \n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Run TrainValidationSplit, and choose the best set of parameters.\n",
    "model50 = tvs.fit(train_validation50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11df3b38-9f9e-4434-b093-56876efbd5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model50.save(\"data/gridsearch-model-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5599d1d9-062c-4e4b-8a7b-bb2d88ed02c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions on test data. model is the model with combination of parameters\n",
    "# that performed best.\n",
    "mypred50 = model50.transform(test50)\\\n",
    "    .select(\"features\", \"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eb0779-fcbc-4439-b7d9-9ef6b6e6f2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results for 50 Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e9c146d2-ddee-4d73-93ac-b429f32d4a75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6450459652706844"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 200 features has failure rate of 0.48\n",
    "# 50 features has failure rate of 0.64\n",
    "mypred50.filter(mypred50.prediction != mypred50.label).count() / mypred50.count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
