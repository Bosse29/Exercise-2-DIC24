{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "501bc548-f2b0-436d-a7a1-6e768c37239e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 1) RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da0a98-0b60-40c1-acbc-823c32128b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Intitialize Scala interpreter\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1df19478-3a3e-4b67-b8e0-5bead0849d30",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataPath: String = hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\n",
       "data_: org.apache.spark.sql.DataFrame = [asin: string, category: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load the data\n",
    "val dataPath= \"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\"\n",
    "val data_ = spark.read.json(dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f24f78cc-6622-4b4f-957c-ba899c071b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairs: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[9] at map at <console>:25\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Extract columns category and reviewText\n",
    "val pairs = data_.rdd.map{row=>\n",
    "    val category = row.getAs[String] (\"category\")\n",
    "    val text = row.getAs[String] (\"reviewText\")\n",
    "    (category,text)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55fa0935-8537-44df-b2be-82f0f40ee8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "stopwordsPath: String = stopwords.txt\n",
       "stopwords: Array[String] = Array(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, bibs, bike, book, books, both, brief, bulbs, but, by, c, came, camera, can, cannot, cant, car, case, cause, ca...\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load stopwords\n",
    "val stopwordsPath = \"stopwords.txt\"\n",
    "val stopwords = sc.textFile(stopwordsPath).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86b28b8b-93c8-468b-9a70-21078c160d7a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pairs_clean: org.apache.spark.rdd.RDD[(String, String)] = MapPartitionsRDD[12] at mapValues at <console>:26\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// extract words only and filter out the stopwords\n",
    "val pairs_clean = pairs.mapValues(value=>value.toLowerCase\n",
    "                                  .split(\"[^a-zA-Z<>^|]+\")\n",
    "                                  .filterNot(x => stopwords.contains(x.toLowerCase)).mkString(\" \")).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4844bf0-cb5e-4119-98a4-3428908fc5f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Int = 2\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Look at the number of partitions\n",
    "pairs_clean.getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faebe3ac-ac37-4053-95af-70c8eefb3a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "categoryTermCount: org.apache.spark.rdd.RDD[((String, String), Int)] = ShuffledRDD[15] at reduceByKey at <console>:25\n",
       "result_A: org.apache.spark.rdd.RDD[(String, (String, Int))] = MapPartitionsRDD[16] at map at <console>:27\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Calculate a number of unique \"category and word\" lines\n",
    "val categoryTermCount = pairs_clean.flatMapValues(x=>x.split(\" \")).map(word => (word, 1)).reduceByKey ((x,y)=>x+y)\n",
    "// Prepare data for the following join per word. The data now represents a tuple (word (category,A = frequency of word and category))\n",
    "val result_A = categoryTermCount.map{case(k,v) => ( (k._2),( k._1, v))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e73d8bb-9482-4d3c-8174-7a92c5e14edf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countWords: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[20] at reduceByKey at <console>:25\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Calculate a number of unique words. The result is a tuple (word, frequency of word )\n",
    "val countWords = pairs_clean.map{ case (key, value) =>  value }.flatMap (x=>x.split(\" \")).map(word => (word, 1)).reduceByKey ((x,y)=>x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35594ecb-84e8-46a4-89bc-2291cc601b74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rddJoin_1: org.apache.spark.rdd.RDD[(String, ((String, Int), Int))] = MapPartitionsRDD[26] at join at <console>:27\n",
       "result_A_B: org.apache.spark.rdd.RDD[(String, (String, Int, Int))] = MapPartitionsRDD[27] at map at <console>:29\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Do the join datasets per word column\n",
    "val rddJoin_1 = result_A.join(countWords)\n",
    "// Prepare data for the following join per category. The data now represents a tuple (category (word, A,B))\n",
    "val result_A_B = rddJoin_1.map{case (k,v)=> (v._1._1, (k, v._1._2, v._2-v._1._2)) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b0c2dbd-b4f7-4802-a830-843d477a9310",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Movies_and_TV,(vecindad,1,0))\n"
     ]
    }
   ],
   "source": [
    "result_A_B.take(1).foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2775e982-8274-4119-98f8-1bb58e6395ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "countCategory: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[29] at reduceByKey at <console>:26\n",
       "rddJoin_2: org.apache.spark.rdd.RDD[(String, ((String, Int, Int), Int))] = MapPartitionsRDD[32] at join at <console>:28\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Calculate a frequency of each category\n",
    "val countCategory = pairs_clean.map{ case (key, value) =>  (key,1)}.reduceByKey ((x,y)=>x+y)\n",
    "// Join per category\n",
    "val rddJoin_2 = result_A_B.join(countCategory).persist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5548ca54-5802-41ad-a2df-4b649c7fd58f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NN: Int = 78829\n",
       "result_A_B_C_D: org.apache.spark.rdd.RDD[(String, (String, Int, Int, Int, Int))] = MapPartitionsRDD[33] at map at <console>:29\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Calculate the number of lines in the whole document\n",
    "val NN = pairs_clean.count().toInt\n",
    "// Calculate A,B,C,D\n",
    "val result_A_B_C_D = rddJoin_2.map{case (k,v)=> (k, (v._1._1, v._1._2, v._1._3, v._2-v._1._2,\n",
    "                                                  NN - v._1._3 - v._2 )) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "729c8835-ec1e-4a85-b5c6-8a5d2c530ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "result_A_B_C_D_chi2: org.apache.spark.rdd.RDD[(String, (Float, String))] = MapPartitionsRDD[34] at map at <console>:26\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Calculate chi_2 value using A,B,C,D,NN\n",
    "val result_A_B_C_D_chi2 = result_A_B_C_D.map{case (k,v)=> {\n",
    "    val A = v._2.toFloat\n",
    "    val B = v._3.toFloat\n",
    "    val C = v._4.toFloat\n",
    "    val D = v._5.toFloat\n",
    "    val top = NN*(A*D-B*C)*(A*D-B*C) \n",
    "    val bottom = (A+B)*(A+C)*(B+D)*(C+D)\n",
    "    val result = top/bottom\n",
    "    (k, (result,v._1))\n",
    "}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "17d84985-ef9c-4519-b36b-648fc3943148",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chi_grouped: org.apache.spark.rdd.RDD[(String, List[(Float, String)])] = MapPartitionsRDD[47] at mapValues at <console>:26\n",
       "chi_grouped_75: org.apache.spark.rdd.RDD[(String, List[(Float, String)])] = ShuffledRDD[51] at sortByKey at <console>:28\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Group the lines according to the key (=category) and sort according to the value of chi_2\n",
    "val chi_grouped = result_A_B_C_D_chi2.groupByKey().mapValues(tuple => tuple.toList.sortBy(-_._1))\n",
    "// Extract the first 75 values in each category\n",
    "val chi_grouped_75 = chi_grouped.mapValues(line=>line.take(75)).sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "358113b7-cdf9-4b7c-ade4-31f59f0d690a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Save the output as a txt file\n",
    "scala.tools.nsc.io.File(\"output_rdd.txt\").writeAll(chi_grouped_75.collect().mkString(\"\\n\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
