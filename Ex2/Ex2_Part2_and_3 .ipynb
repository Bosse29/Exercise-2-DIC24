{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85af0eae-59d8-49f2-972f-c26abd2fc1a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 2 - Text Processing and Classification using Spark\n",
    "## Group 29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8510e59a-1556-4dd9-842e-cbcc464f315b",
   "metadata": {},
   "source": [
    "## Part 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fab9eba1-9e07-4266-84d8-6da799b4185c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://captain01.os.hpc.tuwien.ac.at:9999/proxy/application_1685516010423_1030\n",
       "SparkContext available as 'sc' (version = 3.2.3, master = yarn, app id = application_1685516010423_1030)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, ChiSqSelector}\n",
       "import org.apache.spark.ml.feature.CountVectorizerModel\n",
       "import org.apache.spark.ml.feature.ChiSqSelectorModel\n",
       "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
       "import org.apache.spark.ml.linalg.SparseVector\n",
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{Tokenizer, RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, ChiSqSelector}\n",
    "import org.apache.spark.ml.feature.CountVectorizerModel\n",
    "import org.apache.spark.ml.feature.ChiSqSelectorModel\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.linalg.SparseVector\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c468a8d-20b5-448d-bc75-0cecba907e63",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [asin: string, category: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aeb04b50-9310-4c62-afea-b538d4a9820f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_395435f43919\n",
       "stopwordsPath: String = stopwords.txt\n",
       "stopwords: List[String] = List(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, bibs, bike, book, books, both, brief, bulbs,...\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 1: Tokenize reviewText into unigrams\n",
    "val tokenizer = new Tokenizer()\n",
    "  .setInputCol(\"reviewText\")\n",
    "  .setOutputCol(\"words\")\n",
    "\n",
    "// Step 2: Apply various transformations: stopword removal, and special character removal\n",
    "val stopwordsPath = \"stopwords.txt\"\n",
    "val stopwords = sc.textFile(stopwordsPath).collect().toList\n",
    "val stopwordRemover = new StopWordsRemover()\n",
    "  .setStopWords(stopwords.toArray)\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"filtered_words\")\n",
    "\n",
    "// Step 3: Count the term frequency of each word in the filtered words\n",
    "val countVectorizer = new CountVectorizer()\n",
    "  .setInputCol(stopwordRemover.getOutputCol)\n",
    "  .setOutputCol(\"rawFeatures\")\n",
    "\n",
    "// Step 4: Apply the IDF transformation to get the TF-IDF weighted features\n",
    "val idf = new IDF()\n",
    "  .setInputCol(countVectorizer.getOutputCol)\n",
    "  .setOutputCol(\"tfidfFeatures\")\n",
    "\n",
    "// Step 5: Apply Chi-Square feature selection to select the top 2000 terms overall\n",
    "val selector = new ChiSqSelector()\n",
    "  .setFeaturesCol(idf.getOutputCol)\n",
    "  .setLabelCol(\"overall\")\n",
    "  .setOutputCol(\"selectedFeatures\")\n",
    "  .setNumTopFeatures(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14076980-fda6-48ec-9aeb-f1e398cc645a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipeline: org.apache.spark.ml.Pipeline = pipeline_82665673785a\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipeline = new Pipeline()\n",
    "  .setStages(Array(tokenizer, stopwordRemover, countVectorizer, idf, selector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cee33b3-5551-4458-a586-e406a9c4c332",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pipelineModel: org.apache.spark.ml.PipelineModel = pipeline_82665673785a\n",
       "transformedData: org.apache.spark.sql.DataFrame = [asin: string, category: string ... 13 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pipelineModel = pipeline.fit(df)\n",
    "val transformedData = pipelineModel.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "393d29e7-29b4-4361-86b1-7e5e34009fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selectedTerms: Array[String] = Array(\"\", great, good, it's, love, it., time, don't, -, i'm, recommend, bought, make, easy, back, find, work, made, buy, i've, didn't, found, works, book., nice, lot, makes, doesn't, long, bit, feel, loved, characters, years, give, thought, fit, hard, it,, author, things, set, 2, pretty, thing, price, small, perfect, highly, big, part, purchased, book,, sound, enjoyed, series, end, 3, size, excellent, however,, bad, well., happy, wanted, 5, world, times, that's, me., enjoy, interesting, ordered, you're, isn't, 4, amazon, loves, job, read., problem, order, takes, writing, received, wait, family, top, favorite, couldn't, character, great., short, worked, won't, kind, couple, wonderful, left, cover, money, battery, wasn't, felt, started, fits, purchase, revie...\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Get the selected features/terms\n",
    "val selectedTerms = pipelineModel.stages(4).asInstanceOf[ChiSqSelectorModel]\n",
    "  .selectedFeatures\n",
    "  .map(index => pipelineModel.stages(2).asInstanceOf[CountVectorizerModel].vocabulary(index))\n",
    "\n",
    "// Write the selected terms to the file\n",
    "scala.tools.nsc.io.File(\"output_ds_2.txt\").writeAll(selectedTerms.mkString(\"\\n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580f9e46-a82a-4c1d-934d-0760ce290872",
   "metadata": {},
   "source": [
    "## Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "65164b7a-441d-4040-b70a-8a5e440b85f4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
       "import org.apache.spark.ml.feature.Normalizer\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder, TrainValidationSplit}\n",
       "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
       "import org.apache.spark.ml.feature.StringIndexer\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{LinearSVC, OneVsRest}\n",
    "import org.apache.spark.ml.feature.Normalizer\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder, TrainValidationSplit}\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "98b1aae9-8dbf-4296-a5f7-de35c53a8537",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df: org.apache.spark.sql.DataFrame = [asin: string, category: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = spark.read.json(\"hdfs:///user/dic23_shared/amazon-reviews/full/reviews_devset.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "389f3716-2679-432d-975c-22b84999ae41",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, category: string ... 8 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, category: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = df.randomSplit(Array(0.8, 0.2), seed = 1234L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "559f56d6-747a-4741-89e4-e72569bc3f12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer: org.apache.spark.ml.feature.Tokenizer = tok_9e0dacdd2a24\n",
       "stopwordsPath: String = stopwords.txt\n",
       "stopwords: List[String] = List(a, aa, able, about, above, absorbs, accord, according, accordingly, across, actually, after, afterwards, again, against, ain, album, album, all, allow, allows, almost, alone, along, already, also, although, always, am, among, amongst, an, and, another, any, anybody, anyhow, anyone, anything, anyway, anyways, anywhere, apart, app, appear, appreciate, appropriate, are, aren, around, as, aside, ask, asking, associated, at, available, away, awfully, b, baby, bb, be, became, because, become, becomes, becoming, been, before, beforehand, behind, being, believe, below, beside, besides, best, better, between, beyond, bibs, bike, book, books, both, brief, bulbs,...\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Step 1: Tokenize reviewText into unigrams\n",
    "val tokenizer = new Tokenizer()\n",
    "  .setInputCol(\"reviewText\")\n",
    "  .setOutputCol(\"words\")\n",
    "\n",
    "// Step 2: Apply various transformations: stopword removal, and special character removal\n",
    "// val stopwords = StopWordsRemover.loadDefaultStopWords(\"english\")\n",
    "val stopwordsPath = \"stopwords.txt\"\n",
    "val stopwords = sc.textFile(stopwordsPath).collect().toList\n",
    "val stopwordRemover = new StopWordsRemover()\n",
    "  .setStopWords(stopwords.toArray)\n",
    "  .setInputCol(tokenizer.getOutputCol)\n",
    "  .setOutputCol(\"filtered_words\")\n",
    "\n",
    "// Step 3: Count the term frequency of each word in the filtered words\n",
    "val countVectorizer = new CountVectorizer()\n",
    "  .setInputCol(stopwordRemover.getOutputCol)\n",
    "  .setOutputCol(\"rawFeatures\")\n",
    "\n",
    "// Step 4: Apply the IDF transformation to get the TF-IDF weighted features\n",
    "val idf = new IDF()\n",
    "  .setInputCol(countVectorizer.getOutputCol)\n",
    "  .setOutputCol(\"tfidfFeatures\")\n",
    "\n",
    "// Step 5: Apply Chi-Square feature selection to select the top 2000 terms overall\n",
    "val selector = new ChiSqSelector()\n",
    "  .setFeaturesCol(idf.getOutputCol)\n",
    "  .setLabelCol(\"label\")\n",
    "  .setOutputCol(\"selectedFeatures\")\n",
    "  .setNumTopFeatures(2000)\n",
    "\n",
    "// Step 6: Add vector length normalization\n",
    "val normalizer = new Normalizer()\n",
    "  .setInputCol(\"selectedFeatures\")\n",
    "  .setOutputCol(\"normalizedFeatures\")\n",
    "  .setP(2.0) // L2 norm\n",
    "\n",
    "// Step 7: Convert the \"category\" column to a numeric type\n",
    "val indexer = new StringIndexer()\n",
    "  .setInputCol(\"category\")\n",
    "  .setOutputCol(\"label\")\n",
    "\n",
    "// Define the SVM classifier\n",
    "val svm = new LinearSVC()\n",
    "  //.setLabelCol(\"label\")\n",
    "  // .setFeaturesCol(\"normalizedFeatures\")\n",
    "   .setMaxIter(10)\n",
    "   .setRegParam(0.1)\n",
    "\n",
    "// Step 8 Define the One-vs-Rest classifier\n",
    "val ovr = new OneVsRest()\n",
    "   .setClassifier(svm)\n",
    "   .setFeaturesCol(\"normalizedFeatures\")\n",
    "   .setLabelCol(\"label\")\n",
    "\n",
    "val pipeline_2 = new Pipeline()\n",
    "    .setStages(Array(tokenizer, stopwordRemover, countVectorizer, idf, indexer, selector, normalizer, ovr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "578ecd98-314f-4697-846a-68eec13f15b3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model: org.apache.spark.ml.PipelineModel = pipeline_d37b793f627b\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Fit the pipeline to training documents\n",
    "val model = pipeline_2.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "caa315f1-4ed6-4e57-a23b-b51711fdcf4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predictions: org.apache.spark.sql.DataFrame = [asin: string, category: string ... 17 more fields]\n"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Make predictions on the test data\n",
    "val predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e19b754e-df34-4052-b4c1-9b75348a5b7b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = MulticlassClassificationEvaluator: uid=mcEval_bcd399b031fb, metricName=f1, metricLabel=0.0, beta=1.0, eps=1.0E-15\n",
       "f1: Double = 0.5868747088097422\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"f1\")\n",
    "\n",
    "\n",
    "val f1 = evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60033bb-bd7b-4b73-87da-72e63790d487",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using grid search for parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63f4614f-5602-4427-aa8a-5b80ce41a1ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "downsampledDF: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, category: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Apply downsampling\n",
    "val downsampledDF = df.sample(fraction = 0.5, seed = 1234L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a26686c4-0651-4496-8b03-c69115c0ed0e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train_1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, category: string ... 8 more fields]\n",
       "test_1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [asin: string, category: string ... 8 more fields]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train_1, test_1) = downsampledDF.randomSplit(Array(0.8, 0.2), seed = 1234L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "307f09d8-9620-48a7-95bd-43d8bf70b5b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paramGrid: Array[org.apache.spark.ml.param.ParamMap] =\n",
       "Array({\n",
       "\tlinearsvc_76cea51520dc-maxIter: 10,\n",
       "\tchiSqSelector_b0672845a847-numTopFeatures: 2000,\n",
       "\tlinearsvc_76cea51520dc-regParam: 0.1,\n",
       "\tlinearsvc_76cea51520dc-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_76cea51520dc-maxIter: 10,\n",
       "\tchiSqSelector_b0672845a847-numTopFeatures: 2000,\n",
       "\tlinearsvc_76cea51520dc-regParam: 0.01,\n",
       "\tlinearsvc_76cea51520dc-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_76cea51520dc-maxIter: 10,\n",
       "\tchiSqSelector_b0672845a847-numTopFeatures: 2000,\n",
       "\tlinearsvc_76cea51520dc-regParam: 0.001,\n",
       "\tlinearsvc_76cea51520dc-standardization: true\n",
       "}, {\n",
       "\tlinearsvc_76cea51520dc-maxIter: 10,\n",
       "\tchiSqSelector_b0672845a847-numTopFeatures: 2000,\n",
       "\tlinearsvc_76cea51520dc-regParam: 0.1,\n",
       "\tlinearsvc_76cea51520dc-standardization: false\n",
       "}, {\n",
       "\tlinearsvc_76cea51520d...\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create the parameter grid for the grid search\n",
    "val paramGrid = new ParamGridBuilder()\n",
    "  .addGrid(selector.numTopFeatures, Array(2000, 500))\n",
    "  .addGrid(svm.regParam, Array(0.1, 0.01, 0.001))\n",
    "  .addGrid(svm.standardization, Array(true, false))\n",
    "  .addGrid(svm.maxIter, Array(10, 20))\n",
    "  .build()\n",
    "\n",
    "// Define the evaluator\n",
    "val evaluator_2 = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"f1\")\n",
    "\n",
    "// Perform cross-validation with grid search\n",
    "val cv = new CrossValidator()\n",
    "  .setEstimator(pipeline_2)\n",
    "  .setEvaluator(evaluator_2)\n",
    "  .setEstimatorParamMaps(paramGrid)\n",
    "  .setNumFolds(5) // number of folds for cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e977440-2f98-4a59-90e3-14226e6d3939",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "// Fit the cross-validation model to the training data\n",
    "val cvModel = cv.fit(train_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e1360d-dc0b-43cf-ad97-fea69dabd075",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Get the best model from the cross-validation\n",
    "val bestModel = cvModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7107b6-3d65-4673-a685-e82b6b78f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "// Make predictions on the test data using the best model\n",
    "val predictions = bestModel.transform(test)\n",
    "\n",
    "// Evaluate the predictions using the evaluator\n",
    "val f1Score = evaluator.evaluate(predictions)\n",
    "\n",
    "// Print the F1 score on the test data\n",
    "println(s\"F1 score on the test data: $f1Score\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
